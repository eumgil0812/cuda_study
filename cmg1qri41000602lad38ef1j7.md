---
title: "Segmentation"
datePublished: Sat Sep 27 2025 03:59:06 GMT+0000 (Coordinated Universal Time)
cuid: cmg1qri41000602lad38ef1j7
slug: segmentation

---

# Memory Placement

As shown in the figure, only the segments currently in use are placed in physical memory.  
That is, even if the virtual address space is large and mostly empty (sparse address space), only the required portions are actually allocated in physical memory.

Segments such as the operating system, stack, code, and heap are managed with a **base address** and **bounds (size)**.  
Example: the code segment is placed at physical address 32KB with size 2KB; the heap at 34KB with size 3KB; the stack at 28KB with size 2KB.

---

# Address Translation Examples

* Virtual address 100 (inside the code segment) → offset 100 + code base (32KB) = physical address 32868.
    
* Virtual address 4200 (inside the heap) → since the heap starts at VA 4096, offset = 4200 − 4096 = 104.  
    → offset 104 + heap base (34KB) = physical address 34920.
    

If an address goes beyond the segment size (e.g., past the end of the heap), the hardware detects it → traps into the OS → a **segmentation fault** occurs.

---

# How to Identify Which Segment

The hardware uses the high-order bits of the virtual address to determine the segment.  
Example: 14-bit address with 3 segments → use the top 2 bits:

* `00` → Code
    
* `01` → Heap
    
* `11` → Stack
    

The lower bits are used as the offset within the segment.  
If the offset exceeds the bounds, the access is immediately treated as invalid.

---

# Stack Support

The stack grows **backwards** (towards lower addresses).  
Example: base = 28KB, size = 2KB, growth direction = negative.  
Accessing virtual address 15KB → compute negative offset → translates to physical address 27KB.

Thus, the hardware must also maintain a “growth direction” bit for each segment.

---

# Protection and Sharing

Each segment can have protection bits (read, write, execute).  
Example: Code segment = read/execute only → safe to share across multiple processes.  
Heap/stack = read/write allowed.

---

# Degree of Segmentation

* **Coarse-grained segmentation**: a few large chunks (code, heap, stack).
    
* **Fine-grained segmentation**: systems like Multics supported thousands of segments → more flexible, but requires hardware support (segment tables, etc.).
    

---

# External Fragmentation

Since segments are variable in size, small holes appear in physical memory.  
Example: Even with 24KB free in total, if no contiguous 20KB block exists, a new segment cannot be allocated.

### Solutions

* **Memory compaction**: stop processes, move chunks together, update segment bases → expensive.
    
* **Allocation algorithms**: best-fit, worst-fit, first-fit, buddy system. But fundamentally, fragmentation cannot be eliminated.
    

---

# Summary of Segmentation

**Strengths:**

* Efficient for sparse address spaces (unused regions not mapped to memory).
    
* Fast translation (simple base + offset).
    
* Supports code sharing.
    

**Weaknesses:**

* External fragmentation.
    
* Large segments that are sparsely used waste memory.
    

➡ Therefore, more flexible techniques like **paging** are required.

---

# Memory Return and Free List

When a program calls `free()`, the previously allocated block returns to the **free list**.  
If blocks are only appended to the list, small fragments accumulate → external fragmentation.

Without **coalescing** adjacent free blocks, even if the memory is almost entirely free, small fragments may prevent large requests from being satisfied.  
**Solution**: merge adjacent blocks (coalescing) → recover large contiguous free space.

---

# Growing the Heap

When the heap runs out of space:

* Simple: fail and return `NULL` (limited approach).
    
* Common: request additional physical pages from the OS (e.g., via `sbrk()` in UNIX).  
    → The OS maps new pages into the process address space, updates the heap end pointer, and makes the larger heap available.
    

---

# Basic Allocation Strategies

### Best Fit

* Search the free list for blocks ≥ requested size.
    
* Pick the smallest block.
    
* **Pros**: reduces wasted space.
    
* **Cons**: costly search; often leaves small fragments.
    

### Worst Fit

* Split from the largest block.
    
* **Idea**: keep large blocks intact, avoid small fragments.
    
* **Cons**: performs poorly in practice; can worsen fragmentation.
    

### First Fit

* Scan from the front of the free list, return the first suitable block.
    
* **Pros**: fast (no full scan).
    
* **Cons**: small fragments accumulate at the beginning of the list.
    

### Next Fit

* Variant of First Fit: resume search from where the previous one left off.
    
* **Pros**: distributes search load, prevents early-list pollution.
    
* **Performance**: similar to First Fit.
    

---

# Example

Free list = `[10, 30, 20]`  
Request = `15`

* Best Fit → chooses 20 → result `[10, 30, 5]`
    
* Worst Fit → chooses 30 → result `[10, 15, 20]`
    
* First Fit → also chooses 30 (first block large enough).
    

---

# Advanced Techniques

### Segregated Lists

* Maintain separate pools for specific sizes (e.g., 64-byte objects).
    
* **Pros**: faster allocation, less fragmentation.
    
* Example: Solaris **slab allocator** (Jeff Bonwick) for kernel object caching.
    

### Buddy Allocation

* Memory split only into powers of two.
    
* Blocks repeatedly halved until suitable size found.
    
* Freed blocks can be merged with their “buddy” if also free.
    
* **Pros**: merging is very simple (check one address bit).
    
* **Cons**: internal fragmentation (must round to power-of-two sizes).
    

### Others

* Use data structures (balanced binary trees, splay trees) to reduce search costs.
    
* Multiprocessor-optimized allocators (e.g., **Hoard, jemalloc**) for multi-threaded workloads.
    

---

# Final Summary

* Goal of free-space management: **fast and low fragmentation**.
    
* No perfect solution exists → performance depends heavily on workload.
    
* Modern systems combine specialized allocators (slab, buddy, jemalloc) for efficiency and scalability.